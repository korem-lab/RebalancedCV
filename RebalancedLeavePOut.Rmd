---
title: 
output: 
    html_document:
       theme: yeti
---

<style>
.darkgreen {
  background-color:#577836;
  color: white;
  border: 2px solid black;
  margin: 20px;
  padding: 20px;
} 
</style>

<style>
    body { 
            background-color: #4D251C;
            text-color: whitesmoke;
            color: whitesmoke;
            font-family: Palatino;
            font-size: 12pt;
            margin: 20px;
            padding: 20px;
            }
    a:link {
            color: darkblue; 
            background-color: transparent; 
            }
  a:visited {
            color: darkpurple;
            background-color: transparent;
          }
  a:active {
        color: darkpurple;
        background-color: transparent;
      }
  a:hover {
        color: darkpurple;
        background-color: transparent;
        text-decoration: underline;
      }
</style>

<br>
<center> <h1> rebalancedcv.**RebalancedLeavePOut** </h1> </center>
```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy(c('r', 'python', 'bash'), 
               position = c("top", "right"),
               color='brown',
               tooltip_message = "Copy",
               tooltip_success = "Copied!"
               )
```

<style>
.darkgrey {
  background-color: #333333;
  color: white;
  border: 2px solid #577836;
  margin: 20px;
  padding:10px;
} 
</style>
<div class="darkgrey">
*class* rebalancedcv.**RebalancedLeavePOut**(p)
</div>
<br>

Description
------------

<div class="darkgreen">

Rebalanced Leave-P-Out cross-validator.

Provides train/test indices to split data in train/test sets with subsampling within the training set to ensure that all training folds have identical class balances. This cross-validation tests on all distinct samples of size p, while a remaining n - 2p samples form the training set in each iteration, with an additional p samples used to subsamples from within the training set.

This class is designed to have the same functionality and 
implementation structure as scikit-learn's ``LeavePOut()``

Note: Similarly to what was previously mentioned in scikit-learn's documentation, RebalancedLeavePOut(p) is NOT equivalent to RebalancedKFold(n_splits=n_samples // p) which creates non-overlapping test sets. Due to the high number of iterations which grows combinatorically with the number of samples this cross-validation method can be very costly.

At least 1+p observations per class are needed for `RebalancedLeavePOut`.
</div>


Example
--------

```python
### Observing the indices on a small example dataset
import numpy as np
np.random.seed(1)
from rebalancedcv import RebalancedLeavePOut
X = np.array([[1, 2, 1, 2, 1, 2], [3, 4, 3, 4, 3, 4]]).T
y = np.array([0,1,0,1,0,1])
rloo = RebalancedLeavePOut(p=2)
for i, (train_index, test_index) in enumerate(rloo.split(X, y)):
    print(f"Fold {i}:")
    print(f"  Train: index={train_index}")
    print(f"  Test:  index={test_index}")
```

    Fold 0:
      Train: index=[4 5]
      Test:  index=[0 1]
    Fold 1:
      Train: index=[1 4]
      Test:  index=[0 2]
    Fold 2:
      Train: index=[4 5]
      Test:  index=[0 3]
    Fold 3:
      Train: index=[2 3]
      Test:  index=[0 4]
    Fold 4:
      Train: index=[1 2]
      Test:  index=[0 5]
    Fold 5:
      Train: index=[3 4]
      Test:  index=[1 2]
    Fold 6:
      Train: index=[2 5]
      Test:  index=[1 3]
    Fold 7:
      Train: index=[0 5]
      Test:  index=[1 4]
    Fold 8:
      Train: index=[3 4]
      Test:  index=[1 5]
    Fold 9:
      Train: index=[0 5]
      Test:  index=[2 3]
    Fold 10:
      Train: index=[0 5]
      Test:  index=[2 4]
    Fold 11:
      Train: index=[1 4]
      Test:  index=[2 5]
    Fold 12:
      Train: index=[0 1]
      Test:  index=[3 4]
    Fold 13:
      Train: index=[1 2]
      Test:  index=[3 5]
    Fold 14:
      Train: index=[0 1]
      Test:  index=[4 5]

<br>

Methods
--------

The methods of the `RebalancedLeavePOut` class are designed to enable identical funcitonality to scikit-learn's [`LeavePOut`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeavePOut.html).

<div class="darkgreen">
<b>get_n_splits(X, y, groups=None)</b>
<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      Returns the number of splitting iterations in the cross-validator.

        Parameters
        -----------
        X : array-like of shape (n_samples, n_features)
            Training data, where `n_samples` is the number of samples
            and `n_features` is the number of features.

        y : object
            Needed to maintin class balance consistency.

        groups : object
            Always ignored, exists for compatibility.

        Returns
        -------
        n_splits : int
            Returns the number of splitting iterations in the cross-validator.

<br><br>    
<b>split(X, y, groups=None, seed=None)</b> 
        <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
       Generate indices to split data into training and test set.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data, where `n_samples` is the number of samples
            and `n_features` is the number of features.

        y : array-like of shape (n_samples,)
            The target variable for supervised learning problems.

        groups : array-like of shape (n_samples,), default=None
            Group labels for the samples used while splitting the dataset into
            train/test set.
            
        seed : to enforce consistency in the subsampling

        Yields
        ------
        train : ndarray
            The training set indices for that split.

        test : ndarray
            The testing set indices for that split.    
</div>


<style>
.seealso {
  background-color: #F6B302;
  color: black;
  border: 2px solid black;
  margin: 20px;
  padding: 20px;
} 

l{color: darkgreen}
</style>


<div class="seealso">
**See also:**<br>
[_RebalancedKFold_](RebalancedKFold.html)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
Stratified K-fold iterator with training set rebalancing<br>
[_RebalancedLeaveOneOut_](ebalancedLeaveOneOut.html)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
Leave-one-out iterator with training set rebalancing<br>
<br>
For more background on LeavePOut, refer to the scikit-learn [User Guide](https://scikit-learn.org/stable/modules/cross_validation.html#leave-p-out).
    </div>
